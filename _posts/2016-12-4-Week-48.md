---
layout: post
title: Final Thoughts
---

I'll be honest. From the first few lectures I didn't really expect much out of this course, but as the semester has progressed, Dr. Downing has surprised me. Even though the lectures did not focus as much on object oriented programming as much as I thought the title of the course implied, I found that drilling in details about C++ was incredibly valuable. After this course, I think I can confidently say that I know about as much about C++ as I do about Java. I wouldn't say I'm an expert in either language (JavaScript would be the one I'm closest to being an expert in [I don't care if you hate me for it, it just means more job security for me]), but I'll say I can reason my way through most of the things I'd encounter in a codebase in either language now, plus a few other things I hope to never see in a real codebase, such as:

```java
int foo() {
  try {
    return 1;
  } catch (Exception e) {
    // nothing
  } finally {
    return 2;
  }
}
```

*Fun exercise: without compiling or running the above code, what does `foo()` return?*

*  `1`
*  `2`
* please stop

```java
static {
  System.exit(0);
}
```

*Bonus: if you want to mess with somebody doing a programming assignment for CS312, add this to a random file and watch them struggle to figure out why none of their code runs. (Don't actually do thisÂ though, it's kinda mean.)*

From what I understand, there are three main topics in CS371p: C++, tooling, and object-oriented programming. The lectures and exams focused on C++, while the projects covered C++ and tooling.

Most of the stuff related to object-oriented programming in the course was contained in the readings that we were supposed to do outside of class, but personally, I didn't have the motivation to do them because to pass the quizzes I only really needed to do a surface-level skim through each reading. For my personal learning style, it would have helped to cover the concepts in class, perhaps by going through some examples of how to create good object-oriented designs during lectures.

## Lectures

Lectures focused primarily on teaching C++, especially drilling in details that people may find easy to confuse. The lectures were the most well-thought out and well-executed aspect of the class, and I enjoyed learning about all of the small details. I especially appreciate the pedantry; programming languages are inherently precise and pedantic, so to understand them well, one's understanding of the language must also be precise and pedantic. From reading what others have had to say on their blogs, Dr. Downing's way of encouraging people to participate in the class helped a lot of people pay attention in class and better learn the material. Dr. Downing did an excellent job of making lectures engaging.

As I mentioned above, I would have liked to see more discussion of object oriented programming, particularly the SOLID principles, during lectures. I think that having lectures that go through examples like the readings over the SOLID principles would help drill in an understanding of what they mean and why they are significant. And overall, this would help students think more carefully about their designs, especially since many of us are preparing to work in companies whose codebases are most definitely big enough for good design to matter.

## Quizzes

The quiz questions over readings were generally questions that you could answer correctly with a 5 minute skim of the actual article. Quiz questions over lecture material tended to be application of small details covered in the previous lecture or two. I found the topics that the quizzes covered to generally be fair, even though the average scores on the quizzes were often disappointingly low.

Occasionally, there were a few quiz questions that had poorly written or incomplete instructions. For example, the following (not a real quiz question, but similar to one that we actually encountered) would not be complete:

```cpp
int main() {
  A x;
  A y = x;
  x = y;
}

a. A() A(A) A(A)
b. A() A(A) =(A)
c. A() A() A(A) A(A)
d. A() A() =(A) =(A)
```

The issue here is that it is hard to figure out, without having seen similar questions before, what the answer choices represent or even what the question is asking. A more complete way of stating this problem would be this:

```cpp
What is the output of the following program?

#include <iostream>

using namespace std;

class A {
  A() {
    cout << "A() ";
  }
  A(const A&) {
    cout << "A(A) ";
  }
  A& operator=(const A&) {
    cout << "=(A) ";
  }
};

int main() {
  A x;
  A y = x;
  x = y;
}

a. A() A(A) A(A)
b. A() A(A) =(A)
c. A() A() A(A) A(A)
d. A() A() =(A) =(A)
```

After this issue was pointed out, later quiz questions of this format were more explicit in stating what they were asking. However, I would still like to see an explicitly stated "What is the output of the following program?" in every quiz question like this; in my mind, the question is still not complete and well-defined without it.

## Tests

Contrary to what Dr. Downing claimed about cheat sheets not being useful, I found that having a good cheat sheet made the exams more or less trivial. For the most part, this means writing down the prototypes for all of the mentioned standard library functions, the provided operators for each type of iterator, the prototypes for all of the operator overloads, and the prototypes for `std::vector` and `std::deque`. If you have those things on your cheat sheet, you've paid attention in lecture, and you've made an earnest effort in the projects, both exams are more or less a typing speed test.

As such, the best way to study for an OOP exam is [typing.io](https://typing.io).

I think the tests would benefit from having other types of questions in addition to code writing. I like the idea of having the test be primarily code writing, but I think having a few other types of questions mixed in would be nice. For example, one can imagine conceptual multiple choice questions such as:

> When is a local static variable initialized?
>
> * on scope entry
> * on program entry
> * at compile time
> * the first time the function is called
>
> Which of the following exhibit replacement overriding in C++? Check all that apply.
>
> * constructors
> * destructors
> * virtual methods
> * non-virtual methods
> * finalizers

Additionally, the tests could have short free response sections with questions such as:

> List three ways that a programmer could introduce bugs by allocating memory on the heap as opposed to on the stack.
>
> Explain why a programmer may want to allocate memory on the heap instead of on the stack.
>
> Explain the steps needed to determine the value of `baz` at runtime in the following line of C++ code:
>
> ```cpp
int baz = *p.foo()->bar;
```
>
> Assume that `p` is an instance of a class with a method `foo()` that returns a pointer to a struct with a `bar` field of type `int *`.
>
> What is the difference between the pre-increment and post-increment operators in C++ and when would you use one but not the other?

By having more varied types of questions, the tests would be a more enjoyable experience for people who have different study styles that may not be attuned to designing and writing code under time constraints and without access to external resources. It would also make the tests more comprehensive instead of testing understanding of only the C++ aspect of the class material.

## Speakers

I enjoyed the speakers, particularly JPL but mostly because they demoed HoloLens. There are plenty of opportunities to find cool companies through the events they host within UTCS such as Food for Thought, Rise and Dine, and various info sessions and tech talks, but the personal connection that the speakers that come in OOP have with Dr. Downing makes it that much more cool.

## Projects

The projects were a mixed bag. They are good experience for becoming familiar with idiomatic C++ and learning how to use the various tools introduced in the class, including GNU Make, Travis CI, Doxygen, Google Test, Valgrind, and Gcov. However, I think there is a lot of room for improvement in motivating the use of the tools. Aside from Travis CI and Google Test, which I found useful to convince myself of the correctness of my code, the other tools were more of a burden and chuck-to-the-side-we'll-do-it-half-an-hour-before-the-project's-due sort of deal.

### Doxygen

Code documentation with Doxygen can be useful but not in codebases of this size. Though there were provided examples of what the documentation should look like in Collatz, I think that setting aside part of a lecture to give examples of what makes good documentation and how to write it well would be useful. In particular, it would be helpful to explain:

* Why is documentation important when you can just read the source code?
* What kinds of information should you put in your documentation?
* What kinds of information should you not put in your documentation?
* How can you avoid unnecessary redundancy in your function/method documentation?

To illustrate the importance of the last question, consider the following three docstrings for the same function:

```cpp
/** Checks if a number is zero. */
bool is_zero(int num) {
  return num == 0;
}

/**
 * Checks if <code>num</code> is zero.
 * @param num a number
 * @return <code>true</code> if <code>num</code> is <code>0</code>.
 */
bool is_zero(int num) {
  return num == 0;
}

/**
 * Returns <code>true</code> if <code>num</code> is <code>0</code>, or
 * <code>false</code> otherwise.
 * @param num the number to check
 * @return <code>true</code> if <code>num</code> is <code>0</code>, or
 * <code>false</code> otherwise
 */
bool is_zero(int num) {
  return num == 0;
}
```

Which one seems the most useful? The third is clearly the most complete, but it is also redundant and can be expressed more succinctly. The first one is most succinct and would suffice for such a simple function, but you might want something a bit more detailed if you are writing a library that many people will want to use or if you are describing a more complex function or method.

Explaining the importance of finding the balance between being concise and being comprehensive (or even conforming to verbose documentation requirements) would be of practical help to future engineers. Good documentation makes people's lives easier and will cause people to see you as a better person.

### Google Test

The first project did not motivate the importance of unit testing well. Having a requirement of three unit tests per function/method forces you to write contrived or redundant (i.e. testing the same code path) unit tests since you could achieve the same coverage with half the number of unit tests, particularly in Collatz because most of the functions barely had any branching that needed multiple unit tests to cover. Adding sufficient optimizations may have increased the complexity of some parts of the code to warrant more unit tests, but due to the mathematical properties of the Collatz conjecture, there's not really much you can do in terms of intensive optimization.

(You can add a cache, sure, but adding things to a cache and reading them from a cache don't add very many new code paths to have to verify. I was thinking of stuff more like using SIMD and pthreads, but the overhead of starting multiple threads actually made things run slower for our data size and it's not clear how you can optimize Collatz to use SIMD instructions.)

For the Netflix project, precomputing almost everything beforehand meant that there was very little to actually write unit tests for since the client code that we submitted was very thin and more or less just read in a cache and performed a few simple arithmetic operations on some numbers for each input. Unit tests were not very helpful for that project either.

Even the Allocator project was simple enough that you didn't need that many unit tests to comprehensively cover most or all of the possible code paths, since the allocator spec was just about the simplest free list-based solution you could come up with. No thread safety, no performance bounds or guarantees, and no optimizations for real-world usage. My partner and I ended up manually inlining all of our helper functions because we didn't want to write dozens upon dozens of unit tests that all tested the same code paths and didn't add anything meaningful to the solution.

Extensive unit testing was more helpful for the final two projects since there were lots of tricky edge cases and lots of branching overall. I found that writing unit tests convinced me that my code was working properly and I actually uncovered several bugs in our implementation by writing comprehensive tests for every method. I think Darwin and Life are excellent and challenging projects that not only force students to think about object oriented design but also motivate the need for and usefulness of unit testing.

Overall, I think that it would be good to relax the 3 unit tests per function/method requirement for the first three projects to perhaps 2 unit tests per function/method or even some fixed total number of unit tests for the entire project, since the code overall isn't terribly complex for those projects. As is the case in the real world, the same thing (3 unit tests per function/method) may work great for one project but may not be useful at all for another, so showing that requirements are not mindlessly legalistic but rather adapt to the nature of each individual project would be valuable for future semesters.

### GNU Make

Aside from the handful of students who actually went and looked at the contents of the makefiles for the projects, I don't think most people understood what the makefiles were for or why they were having problems with them at various points throughout the semester. There were some issues with the provided makefiles for the proojects, although those mostly got fixed and later projects ran more smoothly afterwards. I think providing comments in the makefile would encourage students to try to fix Make problems on their own instead of just panicking on Piazza, but that's only part of the solution.

Encouraging students to not only read but also understand error messages from Make would be very helpful towards the independent inquiry objective of tthe class. This could even take the form of spending 10 minutes during a lecture going through what a Make error looks like and what information Make gives you when something goes wrong, e.g. filename, line number, exit status, command that returned an error. By showing students that Make is not just a box of black magic that spews out nonsense that only the TAs can understand, they can debug more effectively and more independently, which will be good for them in the long run.

### Valgrind

We weren't even allowed to use the heap directly in most of the projects, so what's the point of checking for memory leaks? Running everything in Valgrind just added some additional runtime overhead and confused people when they weren't sure if "still reachable" was an acceptable output:

```
==50== LEAK SUMMARY:
==50==    definitely lost: 0 bytes in 0 blocks
==50==    indirectly lost: 0 bytes in 0 blocks
==50==      possibly lost: 0 bytes in 0 blocks
==50==    still reachable: 72,704 bytes in 1 blocks
==50==         suppressed: 0 bytes in 0 blocks
==50== Rerun with --leak-check=full to see details of leaked memory
```

### Travis CI

Travis was helpful and overall I appreciated that we learned how to set it up and use it for regression testing. Though there were some outages, those could not really have been predicted so I can't really fault anybody for that. However, I would like to see a faster official response to third-party service incidents from the instructors in the future. Leaving students in the dark about services that are tied to project requirements is not very fun to be on the other side of.

## Concluding Remarks: How to Pass OOP

Since this blog post is ostensibly aimed towards a future student, here's a simple list of things you can do to pass OOP in a cinch:

* Go to every lecture and pay attention.
* Write the things I mentioned above in the "Tests" section on your cheat sheet.
* Work with a partner on every project you have the option to.
* Do all the extra credit you can. They're free points and not a lot of effort.

## Concluding Remarks: Rust

Throughout the course, during the lectures I was very interested in comparing C++ idioms to those in Rust. A big theme that Dr. Downing emphasizes throughout the course is that object oriented languages tend to push errors that are runtime errors in other languages into compile time errors. This is true, and C++ does a fairly good job at it compared to some of the other languages I've worked with (cough JavaScript), but if you're really serious about pushing errors to compile time, you should really be looking at one of the hottest new kids on the block, Rust.

Rust enforces memory safety at compile time and without a garbage collector. If you don't use any `unsafe` blocks, you can know that any code you write will not contain memory leaks because the compiler knows when to allocate and free everything and will make sure that everything gets freed exactly once when it falls out of scope. Rust also does a lot of other cool things, but I'll leave what exactly those things are as an exercise for the reader.

## Concluding Remarks: Tip of the Week

Dr. Downing,

Thanks for a great semester and an interesting, unique course. All the best with future classes and I'll see you in SWE before I graduate.

![tips fedora](http://i1.kym-cdn.com/photos/images/newsfeed/000/682/880/0d0.gif)
