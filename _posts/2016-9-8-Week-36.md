---
layout: post
title: Postmortem on Project 1
---

**What did you do this past week?** I finished up my Collatz implementation and hung around on Piazza a lot.

**What's in your way?** I missed a quiz on Friday for an interview. Grading for the Collatz project is very opaque so I'm still stressed over it.

**What will you do next week?** I'll take a look at project 2 next week. I should also do the readings for each week... I've forgotten them twice in a row and my grade reflects that.

**Pick of the week:** [Excel Messenger.](http://tristancalderbank.com/2016/09/06/excel-messenger-a-terrible-experiment-in-vba/)

**Tip of the week:** Avoid public humiliation.

<img src="/cs371p-blog/images/mark-all-read.png" alt="Mark all read" title="Mark all read" width="222" height="341">

## Great engineers maintain great standards

When it comes to software engineering, I expect the people I work with to be competent, to always strive to correct and prevent mistakes, and to generally have more experience than me with the tools we use. I have held these expectations through my past two internships, where people have generally met and perhaps even exceeded them. Coming back to UT Austin and taking classes in the computer science department, I still expect the same competence, perfectionism, and expertise from my instructors and classmates.

I believe that Professor Downing has high expectations of us as students. In class this past Wednesday, he called out several students for not having read most of the posts on Piazza in front of the entire class, showing that he expects each ones of us to go above and beyond in engaging and participating in the class. In addition, his idiosyncratic way of calling on people and asking them questions during class shows that he expects us to pay attention in class, be engaged in the lecture, and be able to think on the spot. If I were a professor, I would also expect this much to be true of my students. As a student, I strive to engage and participate in both the lectures and class-related discussions, and I try my best to help other people do their best by helping them out on Piazza when they need it.

I also believe that these high expectations are not a one-way street. I expect the people I work with to be competent, to always strive to correct and prevent mistakes, and to generally have more experience than me with the tools we use, whether they are my co-worker, my boss, or my professor. I have held these expectations through my past two internships, where people have generally met and perhaps even exceeded them. Coming back to UT Austin and taking classes in the computer science department, I still expect the same competence, perfectionism, and expertise from my instructors and classmates.

Furthermore, I believe that future students should be able to come to UTCS and see the faculty exhibit that same competence, perfectionism, and expertise. The faculty of UTCS are role models and should set good examples for students to follow throughout their time at the university and hopefully even well beyond that as they enter the workforce. And that is why I would like to suggest several changes to the way that the Collatz project is set up and executed. I hope that the instructors will be able to improve the project so that students in future semesters will have a better experience, better understand the various tools taught, and work more efficiently with a more modern workflow.

But before I talk about how the Collatz project could be improved, let me start with the parts that I liked.

## What went right

### The Collatz toolkit

I liked the incorporation of several software packages to give students exposure to some of the tools we may encounter in the workplace. And in fact, just this one little conjecture brought us into contact with C++, g++, Git, make, Docker, Travis, Doxygen, gtest, gcov, valgrind, and some of the LLVM tools (clang-check, clang-format). As this course carries the Independent Inquiry flag, I think it is reasonable to believe that we are expected to do a little bit of research outside of class to figure out what all of these things do and how all the parts fit together. Personally, I was already familiar with C++, Git, make, gtest, gcov, and valgrind, and that plus my background knowledge in CS allowed me to get a high-level view of how the different components interacted and what the purpose and role of each component was.

I expected that my classmates would do the same, but many of the posts on Piazza showed that a lot of people treated many of these tools as magic and didn't make an effort to read or interpret any of the output or error messages the tools gave. In particular, I think many people were unfamiliar with `make`. Piazza posts betrayed that many people didn't understand that make simply prints output from other commands that it displays and runs and that one can inspect those commands to see exactly what went wrong. Many people avoided looking at the contents of the various makefiles, instead preferring to try all the makefiles blindly and hope that it would fix whatever issue they were running into.

Many of the questions on Piazza were only as descriptive as "it doesn't work when I run `make test`" or "`make` is giving me errors". Many people also didn't even bother to include the specific error they encountered, which shows either inexperience (not knowing that errors contain a wealth of information can be an honest mistake) or apathy towards independent inquiry in the form of debugging; the first step in debugging is reading and interpreting the error!

It would be ideal if there were a way to cover how to use each one of the tools we encountered in this project in detail. However, I'm fully aware that teaching is incredibly difficult and I know that some ideals are simply not possible given the constraints of reality. I think that all things considered, Professor Downing does a good job of introducing us to the toolkit of the class and his expectation that we explore how to use the various tools on our own is reasonable and exemplifies the Independent Inquiry component of the class.

### Feedback and Suggestions

Professor Downing's insistence on reading everybody's blog posts about OOP and publication of his past professor evaluation scores show that he truly cares about making the class as good as possible and that he wants to make a positive impact on his students' careers and programming habits. He is also very receptive to feedback and suggestions. Throughout the course of the past two weeks, I've made several suggestions that have been implemented: using `.gitignore`, making small tweaks to the `makefile`, and suggested that we use pull requests for submitting tests to the public test repo instead of adding the entire class as collaborators.

No professor I've taken in UTCS has gone this far to demonstrate their willingness to adapt to the rapidly changing landscape of new tools and best practices, and I highly appreciate that the class really *feels* like it's being taught in 2016 as opposed to being stuck in the '80s or '90s. I have taken courses previously that used severely outdated, even if consistent and coherent, tools and standards. Without naming specifics:

* I've taken a class with a website that incorporated exactly zero lines of CSS, distributed files over FTP, and incorporated version tracking in project files solely via comments at the top of each file,
* I've taken a class that made everybody to install an ancient version of Tcl/Tk on a Linux box to compile a tool that was required to complete the projects, and
* I've taken a class where the build system required to compile and run the projects is so arcane  and prone to subtle mistakes that no student in UTCS as far as I know has ever succeeded in recreating it outside of the lab machines and everybody is forced to use the lab machines to accomplish any work on the projects, causing the labs to be overcrowded and smelly through late nights.

Knowing that academia is usually a few decades behind on best practices, it is very refreshing to see a professor actively trying to keep up with the latest tools and trends, including Docker and Travis. This is particularly impressive because neither Docker nor Travis is more than five years old---on an academic timescale, these tools shouldn't even exist yet for another twenty or so years! It is admittedly difficult to know that you're teaching the best tools in the class when the "best" changes every few years, so the fact that these tools have made their way into the class at all is in itself extremely impressive.

I also know that many professors have a preferred method (or function, if they're not object oriented ¯\\_(ツ)_/¯) of teaching that they settle on and never change once they establish that it works reasonably well. Professor Downing, on the other hand, was willing to change the project to match current industry best practices even after the project went out, and I really hope that he will keep these changes and iterate and improve on them to give future students a better experience in the class.

## What went wrong

This begins the postmortem section of the blog post.

I will attempt to follow the example of Google, where I worked this past summer, in examining incidents where things went wrong using [blameless postmortems](http://codeahoy.com/2016/06/20/blameless-postmortems-examining-failure-without-blame/). I think it's fair to say that the Collatz project has not exactly been smooth sailing. My personal experience with the project has been that many issues have come up that have never been problematic in my previous classes. These issues have been incredibly frustrating for me, and judging from other students' posts on Piazza, I am not alone in feeling this way. As such, I think it would be helpful to identify the pain points of the project and suggest practical solutions so that CS371p may be a more delightful and rewarding experience for future students.

### Opaque grading

The most details that any instructor provided on the grading process is that the graders would run their own script and that we would be penalized if we broke it in any way. This was a cause of frustration, as I could not be confident that my code would be accepted even if I ran the acceptance and unit tests and passed all the checks in my makefile. In previous classes, the process has been more transparent; in the extreme case, Dr. Norman's CS439 provides all of the grading scripts along with all test cases that the graders use so that vigilant students can ensure that their code is correct and their submission format is correct. Other coding-based classes, if they did not provide the grading scripts, were more lenient in the submission process since they knew that without the scripts, students could deviate from the expected submission format in subtle ways.

The Collatz project instructions admit that the specifications are necessarily incomplete because it is impossible to think of all of the edge cases. This is also true of submitting code that needs to pass automated validation. Reading the instructions for submitting the code is tedious, and without a centralized place for all the instructions---there were dozens of instructor posts on Piazza about different requirements, and the project page doesn't encompass all of these requirements in detail---it is difficult for us to keep track of what is expected of us. And even if we have all of the instructions in a centralized place, without the exact grading script it is not possible to know whether or not any particular tree will pass the validation. Written out requirements are one thing, but as computer languages are more precise than natural languages, the grading script will necessarily be more strict than the requirements we as students can see. There may be any number of edge cases that look okay with the specification we're shown but the automated grading script rejects, and if that happens, it's the student's grade that suffers.

For that reason, I think it would be very helpful for future students if this class takes one of the approaches of other classes in the department. The first approach would be to provide the grading script. The exact test cases themselves would not be necessary, especially since we have tests from hundreds of other students in the public test repo, but being able to verify that our code will pass at least the initial validation would give students a better state of mind.

In the real world, you will know all of the test cases your code is expected to pass, and furthermore, you will know exactly what command is being run to trigger those test cases and exactly what output that command yields. It would be folly to not provide these to engineers in the workplace. Since projects in this class are meant to model the real world, providing more insight into the grading process would fit right into this model.

As an aside, I think that files that are repository metadata instead of related to the project should be allowed in the submission. These would include `.gitignore` and `.gitmodules` in our case. The inclusion of a `.gitignore` itself, as you may remember, was a student suggestion, and if it had not been suggested and added to the project, a file that is considered best practice to have would be cause for a significant grade penalty. Metadata for the repository and the repository data itself should be treated differently, and repository metadata should always be allowed.

*Disclaimer: Automated grading dinged my code for including a submodule (they're kind of files in a sense), so I have a personal interest in being able to verify my assumptions about what's acceptable and what's not with a more transparent grading process.*

### Maintainers' responsibility

This is the first semester in OOP that the public test repo has been managed using pull requests instead of adding students as collaborators. Since this is a new workflow for the instructors, it is expected that there will be some initial confusion as they become acquainted with how pull requests work. For the Collatz project, this confusion manifested itself in the acceptance of several pull requests that contained invalid tests. And even after the presence of invalid tests was pointed out, new invalid tests were still being accepted up until the day the project was due.

Personally, I assumed that the graders would check the validity of each test before accepting it into the repo. This assumption comes from my experience in the real world and my observations of open source projects: project maintainers are responsible for making sure that their project is in a valid state. Contributions to open source projects are thoroughly reviewed by the project maintainers before they can be accepted into the codebase. At my past internship, you had to get approval from an owner of the part of the code you're changing or adding to in order to submit code; if anything went wrong, that person's name would show up as an approver and they would have to answer to higher-ups on why they accepted that code. There is responsibility that comes with maintaining an open source project, and I expected, perhaps too optimistically, that the graders would go above and beyond in taking that responsibility vigilantly.

To their credit, all of the outstanding issues with invalid tests were fixed before the submission deadline. I could tell that the graders worked extra hard to verify new pull requests once they knew what the common issues were and how to look for them, and I feel kind of bad for them because verifying a pull request, as far as I know, requires cloning the code from downstream, running whatever checks are necessary, commenting on the pull request if there are any issues, and then keeping track of everybody's pull requests. With two hundred students in the class, I can appreciate that that is a pretty big effort.

I think a great way to solve the issue of putting lots of extra work on the graders would be to hook up the public test repo to Travis so that incoming pull requests can be automatically verified without any work required on the graders' part. This would also allow students to get fast feedback. The interactive process of validating a pull request could be automated so that the graders would only have to check that Travis accepts the build to know that the tests are good.

I imagine that in past semesters, the test repository was nearly always in a broken state since students could merge whatever they wanted to. Best practices exist for repositories with this many collaborators: each person should work on a separate branch, and `master` should only be updated via pull requests that are validated by other collaborators rather than individual commits. But I have a lot of doubt that all of two hundred students would do the diligence of learning and adhering to these best practices, especially if Git is new to them. So I appreciate that we are using pull requests, and I hope that now that the instructors are more familiar with the pull request workflow, future projects and semesters can run more smoothly and better reflect the standard workflow that is used in the real world.

### When Travis CI stops working

On the night before the project was due, some students started noticing that Travis was failing builds that had been passing previously. It turns out that they moved all of us to container-based infrastructure, and none of us were able to get any passing builds until the afternoon of the due date.

(I was on a four hour flight by the time another student had gotten a hold of Travis customer support and somehow convinced them to roll back the change so our tests would pass; since I didn't expect to have time to submit my project after I landed, I merged my `dev` changes into `master` on a failing build even though the project specified that the build had to pass to merge into `master`.)

The problem affected all of the students in the class and there were many dozens of reports on Piazza that there was an issue with Travis over the course of more than half a day. Such a big problem would have implications on the grading and submission process. For me, I had to submit my project early, so I had to merge into `master` against the project spec if I wanted to have anything to show at all. I expected an instructor to issue a follow-up on the issue and let us know how it would affect our projects, but there was no instructor response for any of the reports, even after a student asked for an instructor to give an update on Piazza.

If we were a real tech company, this would be considered a high priority outage and somebody would immediately be contacting Travis support and giving real-time status updates as the incident progresses. It's likely that our hypothetical company would have an SLA with Travis---and thus priority support---and whoever is responsible for maintaining contact with Travis would be responsible for fixing the problem as soon as humanly possible.

Using cloud-based services in the classroom is not a cause for concern in itself, but people should be concerned when those cloud-based services go down and students are left in the dark as to what's going on and how the outage will affect them. I believe that in order to leverage services like Travis well, the instructors should be able to make sure that if the services aren't working as they're expecting, they will be able to let their students know and keep them in the loop on the impact that service issues will have on class projects. Better support from the instructors' side would make for a better student experience in the future.

Additionally, as a precaution, I think it would be a good idea to rewrite the class Travis config to run on container-based infrastructure just in case Travis decides to move us all onto it again in the future. There has already been an effort on Piazza to do this, and such a rewrite could potentially avoid running into the same problem in the future.

## Summary: recommended improvements

* Use pull requests for the public test repository, and ensure the graders are familiar with the pull request process as well as the responsibilities that come with being a project maintainer before accepting pull requests.
* Add continuous integration to the test builds, using a trivial implementation of cycle length to check the validity of test ranges and some extra verifications on the format of the acceptance test outputs, including correctness of line endings, whitespace within each line, and newlines at the end of the file.
* Make sure to only accept pull requests that have been confirmed to be 100% valid.
* Properly declare `collatz-tests` as a dependency using a Git submodule, avoiding confusion with people trying to check in their `collatz-tests/EID-*Collatz.*` test files to their private code repo.
* Modify `.travis.yml` to be runnable on container-based infrastructure.
* Add generous inline comments in the makefile to encourage students to read and modify it to suit their needs if necessary.
* Clarify whether the entire base code repo should be cloned into the private code repo or only the contents of the folder `projects/collatz`.
* Include a *stable* version of `clang-check` and `clang-format` in the class Docker image.
* Respond to blocking issues with third-party services, preferably as soon as the issue is discovered but at the very least with a follow-up clarifying how the issue will affect submission and grading.
* Provide the grading scripts, or at least enough of them for us to know exactly what is required, especially when the project description states that anything that breaks the script results in a significant grade penalty.

## Bonus: textual emphasis

There's an interesting phenomenon that comes up when the ratio of bolded to non-bolded words approaches 1:1. If there are **too many words** is boldface **text**, none of the **bolded words** stand out in **relation** to the **rest** of the **text**. And when the ratio is exactly 1:1, the boldface text stands out no more than the regular text, completely defeating the purpose of emphasizing important words. In other words, **if everything is important, then nothing is important.** Typographical conventions recommend using bold, italic, and underline at most sparingly so that words that need to be emphasized actually stand out and that the reader's ability to scan through the text is not diminished. The project specification on the class website exhibits a high ratio of bolded to non-bolded words, and I think that it would be much more effective to reduce the number of bolded words to the minimum needed to bring out crucial concepts and terms.

## Why I do this

I have a long history of suggesting changes and improvements to classes that I take, starting with my first semester at UT. At the end Dr. Novak's CS314 class, I submitted all of my Lisp test code to Dr. Novak so that students who choose to do the homework in Lisp would not be at a disadvantage to students who choose to do the homework in Java. My test cases, which mirror the Java test cases as closely as possible, are still used in the class now.

The next semester, Professor Dickerson had the idea of creating a web-based assembler and simulator for y86, a simple ISA based on x86 syntax that is used to teach low-level concepts such as machine code and processor pipelining. [I created the simulator](https://xsznix.github.io/js-y86/), which helped many students in the class because the y86 simulator that comes with the textbook that teaches it needs a very specific version of Tcl and Tk to compile and the interface is about as modern as a PDP-11 and as intuitive as `ed`, the standard text editor. (I exaggerate, but still.) Plus, my simulator has step-by-step debugging, breakpoints, and inline compile errors, which students who have used an IDE such as Eclipse would be accustomed to.

My primary motivation for wanting to see my classes improve is that I want to see a job well done. It frustrates me when there are problems with my classes that could easily be solved, such as adding Lisp test cases or creating a web-based y86 simulator. Like I mentioned earlier, I have high standards for people in the field of computer science, and I want to see people showing their best in every class, whether as a student or as an instructor. I want students to come to UTCS and see that our program is worth the effort of going through. I want our classes, especially application-based classes, to demonstrate the highest standard of excellence in both planning and execution so that we can continue to distinguish ourselves as a CS department and be a destination that students from all over the world want to come to for a quality CS education.

Professor Downing's OOP class is meant to prepare us for working in the modern world. We learn how to use modern tools, adhere to modern best practices, and work with modern workflows. As I've noted several times previously, teaching a class like this is incredibly difficult because what's considered "modern" changes every other year, so keeping up with the latest industry standards requires a lot of upkeep. Professor Downing has expressed the sentiment that it's never too late to learn something new, and his receptiveness to suggestions shows that he is serious about making the class as good as possible. To that end, I want to offer anything I can, whether it be suggestions or even code contributions, so that this class continues to be a success and future students will enjoy a class experience that's better than what we have today.
